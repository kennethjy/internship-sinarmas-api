{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9abb87f1-8a0b-4c70-bed8-bd88b931f888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install qwen_vl_utils\n",
    "# !pip install sentence-transformers faiss-cpu transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b8e97a5-dcc1-4210-952a-3945ed6eaeb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GPU Memory: 24576 MB\n",
      "Used GPU Memory: 8672 MB\n",
      "Free GPU Memory: 15474 MB\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def check_cuda_memory():\n",
    "    try:\n",
    "        output = subprocess.check_output([\"nvidia-smi\", \"--query-gpu=memory.total,memory.used,memory.free\", \"--format=csv,nounits,noheader\"])\n",
    "        total, used, free = map(int, output.decode(\"utf-8\").strip().split(\"\\n\")[0].split(\", \"))\n",
    "        print(f\"Total GPU Memory: {total} MB\")\n",
    "        print(f\"Used GPU Memory: {used} MB\")\n",
    "        print(f\"Free GPU Memory: {free} MB\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "check_cuda_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc32e57a-5d21-49e2-9077-0920ac30f800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7 documents about oranges.\n"
     ]
    }
   ],
   "source": [
    "# Testing with local dataset of oranges. Why not banana? Later ;3\n",
    "documents = [\n",
    "    \"Oranges are citrus fruits rich in vitamin C.\",\n",
    "    \"An orange tree can live for 20 to 30 years, but some can reach 100 years.\",\n",
    "    \"Orange juice is a popular breakfast drink worldwide.\",\n",
    "    \"Oranges contain antioxidants that boost the immune system.\",\n",
    "    \"Brazil is the world's largest producer of oranges.\",\n",
    "    \"The peel of an orange can be used for essential oils and flavoring.\",\n",
    "    \"Oranges originated in Southeast Asia thousands of years ago.\"\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents about oranges.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "972f3fb2-5e83-4df6-9e4a-1a62cfe11b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 7 embeddings.\n"
     ]
    }
   ],
   "source": [
    "# Convert documents to embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "document_embeddings = [embed_model.encode(doc) for doc in documents]\n",
    "\n",
    "print(f\"Generated {len(document_embeddings)} embeddings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79efa21f-5006-4c68-a5a9-a9d2cc6ff3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored document embeddings in FAISS vector database.\n"
     ]
    }
   ],
   "source": [
    "# Convert embeddings to NumPy array and add to FAISS index\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "dimension = len(document_embeddings[0])  # Vector size \n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "vectors = np.array(document_embeddings, dtype=\"float32\")\n",
    "index.add(vectors)\n",
    "\n",
    "print(\"Stored document embeddings in FAISS vector database.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1df8d92-311e-4d55-b9cc-2a18c51c18a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cabff0c0f3f341a5a4ce220e50f98f3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Code from https://github.com/QwenLM/Qwen2.5-VL, adjusted with 3B version\n",
    "\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import torch\n",
    "import os\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# default: Load the model on the available device(s)\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-3B-Instruct\", \n",
    "    torch_dtype=torch.bfloat16, \n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c9205a9-d95b-49fe-b261-3c4526cf9d2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Oranges are typically grown on trees that can live for several decades to over 100 years. The lifespan of an orange tree depends on various factors such as climate, soil quality, and care practices.\\n\\nAs for who produces oranges, they are primarily produced by farmers and orchardists around the world. Some of the major producers include China, India, Brazil, and the United States.\\n\\nBrazil is indeed one of the largest producers of oranges in the world, with a significant portion of its production coming from the state of SÃ£o Paulo. However, it's important to note that other countries also produce large quantities of oranges, making Brazil just one of many major producers globally.\"]\n"
     ]
    }
   ],
   "source": [
    "# No RAG\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\")\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                # \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n",
    "                # \"image\": \"color_img/color_img_7.png\",\n",
    "                \"image\": \"orange.jpg\",\n",
    "                # \"image\": \"flowchart_img/flowchart_1.png\",\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"This is an orange. How long can their trees live for? And who is producing them? Is Brazil the largest producer?\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(model.device)\n",
    "\n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=512)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "052a67d0-c407-4cd5-af1e-40bdfd6c27a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DEBUG] Retrieved Context:\n",
      "1. Brazil is the world's largest producer of oranges.\n",
      "2. An orange tree can live for more than 69 years.\n",
      "\n",
      "=== ðŸ’¬ AI Response ===\n",
      "system\n",
      "Use the following information to enhance your answer.\n",
      "user\n",
      "This is an orange. How long can their trees live for? And who is producing them? Is Brazil the largest producer?\n",
      "assistant\n",
      "Orange trees can live for several decades, with some reaching up to 50 years or more under ideal conditions. The lifespan of an orange tree depends on factors such as climate, soil quality, and care practices.\n",
      "\n",
      "Oranges are typically grown by farmers and orchardists around the world. They are a popular crop in many countries, including Brazil, which is indeed one of the largest producers of oranges globally. Brazil's production is significant due to its favorable climate, large agricultural land, and advanced technology in citrus farming.\n",
      "=======================\n"
     ]
    }
   ],
   "source": [
    "# With RAG\n",
    "from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# Function to retrieve the most relevant document based on the user's actual question\n",
    "def retrieve_relevant_text(user_question, top_k=2):\n",
    "    query_embedding = embed_model.encode(user_question).reshape(1, -1)\n",
    "    _, retrieved_indices = index.search(query_embedding, top_k)\n",
    "    retrieved_docs = [documents[i] for i in retrieved_indices[0]]\n",
    "    return retrieved_docs\n",
    "\n",
    "# SET USER QUERY HERE\n",
    "user_question = \"This is an orange. How long can their trees live for? And who is producing them? Is Brazil the largest producer?\"\n",
    "\n",
    "# Use the user's actual question as the query\n",
    "retrieved_context = retrieve_relevant_text(user_question)\n",
    "\n",
    "# Debug output: Print retrieved context separately\n",
    "print(\"\\n[DEBUG] Retrieved Context:\")\n",
    "for i, fact in enumerate(retrieved_context, 1):\n",
    "    print(f\"{i}. {fact}\")\n",
    "\n",
    "# Combine retrieved context into a single string for input\n",
    "retrieved_text = \" \".join(retrieved_context)\n",
    "\n",
    "# Prepare chat message with retrieved context and image\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Use the following information to enhance your answer.\"},\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\n",
    "            \"type\": \"image\",\n",
    "            \"image\": \"orange.jpg\",\n",
    "        },\n",
    "        {\"type\": \"text\", \"text\": f\"{user_question}\"},\n",
    "    ]}\n",
    "]\n",
    "\n",
    "# Convert to model input\n",
    "text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "# Generate response\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=512)\n",
    "response = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "# Print final output in a readable format\n",
    "print(\"\\n=== ðŸ’¬ AI Response ===\")\n",
    "print(response[0])\n",
    "print(\"=======================\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d278067-d26e-4532-a489-b0613f617c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GPU Memory: 24576 MB\n",
      "Used GPU Memory: 16873 MB\n",
      "Free GPU Memory: 7273 MB\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def check_cuda_memory():\n",
    "    try:\n",
    "        output = subprocess.check_output([\"nvidia-smi\", \"--query-gpu=memory.total,memory.used,memory.free\", \"--format=csv,nounits,noheader\"])\n",
    "        total, used, free = map(int, output.decode(\"utf-8\").strip().split(\"\\n\")[0].split(\", \"))\n",
    "        print(f\"Total GPU Memory: {total} MB\")\n",
    "        print(f\"Used GPU Memory: {used} MB\")\n",
    "        print(f\"Free GPU Memory: {free} MB\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "check_cuda_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcd6384-a89f-4a41-aa87-cfd5e528228d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
