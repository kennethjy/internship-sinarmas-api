{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5d6dcaa-6e8b-4df4-9642-c389c37af473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install faiss-cpu sentence-transformers PyMuPDF torchvision\n",
    "# !pip install git+https://github.com/openai/CLIP.git \n",
    "# Note that CLIP can't be downloaded directly. Requires a git clone of the link. \n",
    "# It is a multi-modal AI model developed by OpenAI that can understand and relate text and images. \n",
    "# It allows us to encode images and text into a shared embedding space, making it useful for image retrieval, classification, and search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c45815e3-2d4e-432e-a18d-099e6e919679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "import fitz  \n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration\n",
    "\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "711ce78c-a5c3-4564-b835-e4b32ee5fd57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GPU Memory: 24576 MB\n",
      "Used GPU Memory: 918 MB\n",
      "Free GPU Memory: 23229 MB\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def check_cuda_memory():\n",
    "    try:\n",
    "        output = subprocess.check_output([\"nvidia-smi\", \"--query-gpu=memory.total,memory.used,memory.free\", \"--format=csv,nounits,noheader\"])\n",
    "        total, used, free = map(int, output.decode(\"utf-8\").strip().split(\"\\n\")[0].split(\", \"))\n",
    "        print(f\"Total GPU Memory: {total} MB\")\n",
    "        print(f\"Used GPU Memory: {used} MB\")\n",
    "        print(f\"Free GPU Memory: {free} MB\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "check_cuda_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a0a007f-be43-40b7-8cae-0281bed0a839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "385d277189424a849eec470a7c2ba6f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Qwen2.5-VL-3B model loaded successfully!\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# Free up memory before running Qwen\n",
    "torch.cuda.empty_cache()  \n",
    "gc.collect()\n",
    "\n",
    "# Load text embedding model\n",
    "text_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Load CLIP model for image embeddings\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "# clip_model, preprocess = clip.load(\"ViT-B/32\", device=\"cpu\")\n",
    "\n",
    "# Load Qwen2.5-VL-3B model & processor\n",
    "qwen_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-3B-Instruct\", \n",
    "    torch_dtype=torch.bfloat16,  # bfloat16 is more memory-efficient\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\"  # Auto-manages CPU/GPU offloading\n",
    ")\n",
    "\n",
    "qwen_processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\")\n",
    "\n",
    "print(\"✅ Qwen2.5-VL-3B model loaded successfully!\")\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "deebed08-7271-4252-998a-8a945099d250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GPU Memory: 24576 MB\n",
      "Used GPU Memory: 9115 MB\n",
      "Free GPU Memory: 15031 MB\n"
     ]
    }
   ],
   "source": [
    "import subprocess \n",
    "\n",
    "def check_cuda_memory():\n",
    "    try:\n",
    "        output = subprocess.check_output([\"nvidia-smi\", \"--query-gpu=memory.total,memory.used,memory.free\", \"--format=csv,nounits,noheader\"])\n",
    "        total, used, free = map(int, output.decode(\"utf-8\").strip().split(\"\\n\")[0].split(\", \"))\n",
    "        print(f\"Total GPU Memory: {total} MB\")\n",
    "        print(f\"Used GPU Memory: {used} MB\")\n",
    "        print(f\"Free GPU Memory: {free} MB\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "check_cuda_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e51ee44-c3f1-43a8-9dfc-6da7e08dc60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text from 1.pdf\n",
      "Extracted text from 2.pdf\n",
      "Extracted text from 3.pdf\n",
      "Extracted text from 4.pdf\n",
      "Extracted text from 5.pdf\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a given PDF file.\"\"\"\n",
    "    text = \"\"\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        for page in doc:\n",
    "            text += page.get_text(\"text\") + \"\\n\"\n",
    "    return text.strip()\n",
    "\n",
    "# Define directory paths\n",
    "base_dir = \"business_flowcharts\"\n",
    "pdf_dir = os.path.join(base_dir, \"documents\")\n",
    "image_dir = os.path.join(base_dir, \"flowcharts\")\n",
    "\n",
    "# Read all PDFs and extract text\n",
    "pdf_texts = {}\n",
    "for file in sorted(os.listdir(pdf_dir)):  \n",
    "    if file.endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(pdf_dir, file)\n",
    "        text = extract_text_from_pdf(pdf_path)\n",
    "        pdf_texts[file] = text\n",
    "        print(f\"Extracted text from {file}\")\n",
    "\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "087b7954-7e25-4442-accf-fcc03a205649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for p in pdf_texts.values():\n",
    "#     print(p)\n",
    "#     print(\"===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5778a2e0-bef6-41dc-81e4-cbeaad613b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored document text embeddings in FAISS database.\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Convert extracted PDF texts to embeddings\n",
    "pdf_embeddings = []\n",
    "pdf_filenames = list(pdf_texts.keys())\n",
    "\n",
    "for filename in pdf_filenames:\n",
    "    embedding = text_model.encode(pdf_texts[filename])\n",
    "    pdf_embeddings.append(embedding)\n",
    "\n",
    "# Convert list to NumPy array for FAISS\n",
    "pdf_embeddings = np.array(pdf_embeddings, dtype=\"float32\")\n",
    "\n",
    "# Create FAISS index for text embeddings\n",
    "text_index = faiss.IndexFlatL2(pdf_embeddings.shape[1])\n",
    "text_index.add(pdf_embeddings)\n",
    "\n",
    "print(\"Stored document text embeddings in FAISS database.\")\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e29ad44b-3d4d-47c6-953a-6db788949f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored flowchart image embeddings in FAISS database.\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "def get_image_embedding(image_path):\n",
    "    \"\"\"Generate an embedding for a flowchart image using CLIP.\"\"\"\n",
    "    image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        embedding = clip_model.encode_image(image).cpu().numpy()\n",
    "    return embedding.flatten()\n",
    "\n",
    "# Process and store embeddings for flowchart images\n",
    "image_embeddings = []\n",
    "image_filenames = []\n",
    "\n",
    "for file in sorted(os.listdir(image_dir)):  # Ensure order matches PDFs\n",
    "    if file.endswith(\".png\"):\n",
    "        image_path = os.path.join(image_dir, file)\n",
    "        embedding = get_image_embedding(image_path)\n",
    "        image_embeddings.append(embedding)\n",
    "        image_filenames.append(file)\n",
    "\n",
    "# Convert list to NumPy array for FAISS\n",
    "image_embeddings = np.array(image_embeddings, dtype=\"float32\")\n",
    "\n",
    "# Create FAISS index for image embeddings\n",
    "image_index = faiss.IndexFlatL2(image_embeddings.shape[1])\n",
    "image_index.add(image_embeddings)\n",
    "\n",
    "print(\"Stored flowchart image embeddings in FAISS database.\")\n",
    "\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f77f6d38-0abe-48f4-a448-4782acc8b1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# No RAG\n",
    "# Function to query Qwen with both text and an image\n",
    "def query_qwen_with_image(image_path, query_text):\n",
    "    \"\"\"Send an image and text query to Qwen2.5-VL-3B.\"\"\"\n",
    "    \n",
    "    # Load the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # Define the user message\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": image},\n",
    "                {\"type\": \"text\", \"text\": query_text},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Format input for Qwen\n",
    "    text = qwen_processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = qwen_processor(\n",
    "        text=[text],\n",
    "        images=[image],  # Provide the image input\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(qwen_model.device)\n",
    "\n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        output_ids = qwen_model.generate(**inputs, max_new_tokens=512)\n",
    "\n",
    "    # Decode response\n",
    "    response_text = qwen_processor.batch_decode(output_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    return response_text\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb51f392-80bc-40b0-bab6-f0613aa993a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Qwen's Response:\n",
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "In this order processing flowchart, who prepares and packs the order upon a successful payment?\n",
      "assistant\n",
      "The order is packed by the system or the relevant department after the payment is processed successfully.\n"
     ]
    }
   ],
   "source": [
    "# NO RAG\n",
    "query_text = \"In this order processing flowchart, who prepares and packs the order upon a successful payment?\"\n",
    "image_path = \"business_flowcharts/flowcharts/4.png\"  # Flowchart image here\n",
    "qwen_response = query_qwen_with_image(image_path, query_text)\n",
    "print(f\"🤖 Qwen's Response:\\n{qwen_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "709b407b-cf75-4f07-9ba2-299101492a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# With RAG\n",
    "# Function to Retrieve Relevant PDFs & Flowcharts\n",
    "def retrieve_relevant_data(query, top_k=2):\n",
    "    \"\"\"Retrieve the most relevant documents & images for the query using FAISS.\"\"\"\n",
    "    \n",
    "    # Convert query to text embedding\n",
    "    query_embedding = text_model.encode(query).reshape(1, -1)\n",
    "    \n",
    "    # Search FAISS text database (Retrieve relevant PDFs)\n",
    "    _, text_results = text_index.search(query_embedding, top_k)\n",
    "    retrieved_pdfs = [pdf_filenames[idx] for idx in text_results[0]]\n",
    "    \n",
    "    # Convert query to image embedding using CLIP\n",
    "    text_tokenized = clip.tokenize([query]).to(device)\n",
    "    with torch.no_grad():\n",
    "        image_query_embedding = clip_model.encode_text(text_tokenized).cpu().numpy()\n",
    "    \n",
    "    # Search FAISS image database (Retrieve relevant flowcharts)\n",
    "    _, image_results = image_index.search(image_query_embedding, top_k)\n",
    "    retrieved_images = [image_filenames[idx] for idx in image_results[0]]\n",
    "\n",
    "    return retrieved_pdfs, retrieved_images\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b7051d7-fb87-4396-8dbc-da87848708b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Function to Query Qwen with RAG (Flowchart Context)\n",
    "def query_qwen_with_rag(query, flowchart_img, top_k=2):\n",
    "    \"\"\"Retrieve relevant flowchart data (text & images) and query Qwen for an AI-generated response.\"\"\"\n",
    "    \n",
    "    # Retrieve relevant PDFs (text) & Flowcharts (images)\n",
    "    retrieved_pdfs, retrieved_images = retrieve_relevant_data(query, top_k)\n",
    "    \n",
    "    # Extract text from retrieved PDFs\n",
    "    context = \"\\n\".join([pdf_texts[pdf] for pdf in retrieved_pdfs])\n",
    "    \n",
    "    # Load the provided flowchart image\n",
    "    image = Image.open(flowchart_img).convert(\"RGB\")\n",
    "    \n",
    "    # Define the user message (Injecting retrieved context)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": image},  # Provide the provided image\n",
    "                {\"type\": \"text\", \"text\": f\"Here is a related document as context to answer the user's query.\\n\\nContext:\\n{context}\\n\\nQuery: {query}\"},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Format input for Qwen\n",
    "    text = qwen_processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = qwen_processor(\n",
    "        text=[text],\n",
    "        images=[image],  # Provide the image input\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(qwen_model.device)\n",
    "    \n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        output_ids = qwen_model.generate(**inputs, max_new_tokens=512)\n",
    "    \n",
    "    # Decode response\n",
    "    response_text = qwen_processor.batch_decode(output_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    return response_text, retrieved_pdfs, retrieved_images\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2987ff4-0176-41a1-bd10-ed5febf88f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define query and flowchart image\n",
    "# query = \"Can you see the flowchart i provided here about customer support? What if further assistance is needed?\"\n",
    "# flowchart_img = \"business_flowcharts/flowcharts/1.png\"\n",
    "\n",
    "# # Query Qwen with RAG\n",
    "# qwen_response_rag, retrieved_pdfs, retrieved_images = query_qwen_with_rag(query, flowchart_img)\n",
    "\n",
    "# # Print results\n",
    "# print(f\"\\n🤖 Qwen's Response (With RAG):\\n{qwen_response_rag}\")\n",
    "# print(f\"🔍 Retrieved Documents: {retrieved_pdfs}\")\n",
    "# print(f\"🖼️ Retrieved Flowcharts: {retrieved_images}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4183f034-dc03-41f9-8aa5-8cb5bdc6d5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🤖 Qwen's Response (With RAG):\n",
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "Here is a related document as context to answer the user's query.\n",
      "\n",
      "Context:\n",
      "Order Processing Flowchart\n",
      "This flowchart describes the process of handling a customer order, from placement to delivery. A\n",
      "well-structured order processing system is crucial for ensuring that customers receive their products\n",
      "or services on time and with minimal issues. This process involves checking payment, packing,\n",
      "shipping, and confirming order receipt. Proper handling of each step helps improve customer\n",
      "satisfaction and prevents delays, refunds, or disputes.\n",
      "Step-by-Step Process:\n",
      "- The customer places an order.\n",
      "The order process starts when a customer places an order through an online store, phone call, or\n",
      "physical store. The order is logged in the company's system for further processing.\n",
      "- The system checks if the payment is processed.\n",
      "The system verifies if the customer's payment has been processed successfully. This can include\n",
      "credit card payments, digital wallets, or bank transfers.\n",
      "- If payment fails, request payment again.\n",
      "If the payment fails due to insufficient funds, incorrect details, or processing errors, the system\n",
      "requests the customer to retry payment or use an alternative payment method.\n",
      "- If payment is successful, the order is packed.\n",
      "If payment is successful, the warehouse team prepares and packs the order to ensure safe\n",
      "transportation.\n",
      "- The packed order is shipped.\n",
      "The packed order is handed over to a shipping carrier (e.g., UPS, FedEx, DHL) for delivery.\n",
      "- If shipping fails, handle shipping issues.\n",
      "If there are shipping issues such as incorrect address or lost package, the logistics team\n",
      "investigates and resolves them.\n",
      "\n",
      "- The order is delivered to the customer.\n",
      "Once the order reaches the customer's location, it is marked as delivered.\n",
      "- The system asks for customer confirmation.\n",
      "The system asks for the customer to confirm receipt of the order, usually via email or app\n",
      "notification.\n",
      "- If confirmed, the order is completed.\n",
      "If the customer confirms receiving the order and is satisfied, the order is officially completed.\n",
      "- If not confirmed, handle delivery issues.\n",
      "If the customer reports missing or damaged items, the company initiates an issue resolution\n",
      "process, which may include replacing the order or issuing a refund.\n",
      "Purchase Requisition Flowchart\n",
      "This flowchart describes the process of requesting, reviewing, and approving purchases in a\n",
      "business environment. A purchase requisition is an internal document that an employee submits to\n",
      "request approval for purchasing goods or services necessary for business operations. The process\n",
      "ensures that purchases are justified, within budget, and compliant with company policies before\n",
      "proceeding to supplier selection and payment.\n",
      "Step-by-Step Process:\n",
      "- An employee submits a purchase request.\n",
      "An employee identifies the need for a product or service and submits a formal purchase request to\n",
      "the procurement department.\n",
      "- The request is reviewed by a manager.\n",
      "A manager or supervisor reviews the request to determine its necessity and alignment with company\n",
      "goals.\n",
      "- If the budget is available, the request is approved.\n",
      "If the company's budget allows for the purchase, approval is granted, and the process moves to the\n",
      "next step.\n",
      "- If the budget is not available, the request is rejected.\n",
      "If the budget is insufficient or the request is deemed unnecessary, the purchase request is rejected,\n",
      "and the employee is notified.\n",
      "- The procurement team finds a suitable supplier.\n",
      "The procurement team researches available suppliers who can provide the requested items or\n",
      "services.\n",
      "- Quotes are requested and the best supplier is selected.\n",
      "Multiple quotes are requested from different suppliers to compare pricing, delivery timelines, and\n",
      "\n",
      "service quality.\n",
      "- A purchase order (PO) is issued.\n",
      "Based on the quotes, the most suitable supplier is selected, and a purchase order (PO) is issued to\n",
      "formalize the transaction.\n",
      "- The supplier delivers the goods.\n",
      "The supplier fulfills the order and delivers the goods or services as per the agreed terms.\n",
      "- The invoice is verified and approved.\n",
      "The accounts payable team verifies the supplier's invoice against the purchase order and delivery\n",
      "records to ensure accuracy.\n",
      "- The payment is processed, completing the purchase requisition.\n",
      "Once verified, the payment is processed, marking the completion of the purchase requisition\n",
      "process.\n",
      "\n",
      "Query: In this order processing flowchart, who prepares and packs the order upon a successful payment?\n",
      "assistant\n",
      "In this order processing flowchart, the warehouse team prepares and packs the order upon a successful payment.\n",
      "🔍 Retrieved Documents: ['4.pdf', '5.pdf']\n",
      "🖼️ Retrieved Flowcharts: ['5.png', '3.png']\n",
      "⏳ Execution Time: 0.70 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# user_query = \"In this Employee Onboarding flowchart, what will the employees learn during orientation? Is additional training provided should it be needed?\"\n",
    "# user_query = \"What can you tell me about this flowchart? I want all the details.\"\n",
    "user_query = \"In this order processing flowchart, who prepares and packs the order upon a successful payment?\"\n",
    "flowchart_img = \"business_flowcharts/flowcharts/4.png\"\n",
    "\n",
    "# Measure execution time\n",
    "start_time = time.time()\n",
    "\n",
    "# Query Qwen with RAG\n",
    "qwen_response_rag, retrieved_pdfs, retrieved_images = query_qwen_with_rag(user_query, flowchart_img)\n",
    "\n",
    "# Calculate total time taken\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Print results\n",
    "print(f\"\\n🤖 Qwen's Response (With RAG):\\n{qwen_response_rag}\")\n",
    "print(f\"🔍 Retrieved Documents: {retrieved_pdfs}\")\n",
    "print(f\"🖼️ Retrieved Flowcharts: {retrieved_images}\")\n",
    "print(f\"⏳ Execution Time: {execution_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152f40c0-2751-43d3-922d-3f5f26888f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
