{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc549157-03fd-47f4-b7ee-e1d78e3d5ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Stored document text embeddings in FAISS database.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "import fitz  \n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "### DOCUMENT PROCESSING ###\n",
    "\n",
    "# Load text embedding model\n",
    "text_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a given PDF file.\"\"\"\n",
    "    text = \"\"\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        for page in doc:\n",
    "            text += page.get_text(\"text\") + \"\\n\"\n",
    "    return text.strip()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Enhance retrieval by applying TF-IDF weighting.\"\"\"\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=5000)\n",
    "    X = vectorizer.fit_transform([text]).toarray()\n",
    "    return \" \".join(vectorizer.get_feature_names_out())\n",
    "\n",
    "# Directory setup\n",
    "base_dir = \"business_flowcharts\"\n",
    "pdf_dir = os.path.join(base_dir, \"documents\")\n",
    "\n",
    "# Read and process PDFs\n",
    "pdf_texts = {}\n",
    "pdf_filenames = sorted([f for f in os.listdir(pdf_dir) if f.endswith(\".pdf\")])\n",
    "\n",
    "pdf_embeddings = []\n",
    "for file in pdf_filenames:\n",
    "    pdf_path = os.path.join(pdf_dir, file)\n",
    "    raw_text = extract_text_from_pdf(pdf_path)\n",
    "    processed_text = preprocess_text(raw_text)\n",
    "    pdf_texts[file] = raw_text\n",
    "    embedding = text_model.encode(processed_text)\n",
    "    pdf_embeddings.append(embedding)\n",
    "\n",
    "# Convert to FAISS-compatible format\n",
    "pdf_embeddings = np.array(pdf_embeddings, dtype=\"float32\")\n",
    "pdf_embeddings /= np.linalg.norm(pdf_embeddings, axis=1, keepdims=True)  # Normalize (IMPORTANT)\n",
    "\n",
    "# Create FAISS index\n",
    "text_index = faiss.IndexFlatL2(pdf_embeddings.shape[1])\n",
    "text_index.add(pdf_embeddings)\n",
    "\n",
    "print(\"✅ Stored document text embeddings in FAISS database.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "234472c5-6981-489f-afda-da8f1d784f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Stored flowchart image embeddings in FAISS database.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "### FLOWCHART IMG PROCESSING ###\n",
    "\n",
    "# Load CLIP model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    \"\"\"Preprocess an image dynamically while maintaining aspect ratio.\"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # Resize while keeping aspect ratio\n",
    "    aspect_ratio = image.width / image.height\n",
    "    if aspect_ratio > 1:\n",
    "        new_width = 224\n",
    "        new_height = int(224 / aspect_ratio)\n",
    "    else:\n",
    "        new_height = 224\n",
    "        new_width = int(224 * aspect_ratio)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((new_height, new_width)),  # Maintain aspect ratio\n",
    "        transforms.Pad((0, 0, 224 - new_width, 224 - new_height), fill=(255, 255, 255)),  # Pad with white\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.481, 0.457, 0.408], std=[0.268, 0.261, 0.275]),  # CLIP normalization (ALSO IMPORATNT)\n",
    "    ])\n",
    "\n",
    "    return transform(image).unsqueeze(0)\n",
    "\n",
    "def get_image_embedding(image_path):\n",
    "    \"\"\"Generate an embedding for a flowchart image using CLIP.\"\"\"\n",
    "    image_tensor = preprocess_image(image_path).to(device)\n",
    "    with torch.no_grad():\n",
    "        embedding = clip_model.encode_image(image_tensor).cpu().numpy()\n",
    "    return embedding.flatten()\n",
    "\n",
    "# Directory setup\n",
    "image_dir = os.path.join(base_dir, \"flowcharts\")\n",
    "image_filenames = sorted([f for f in os.listdir(image_dir) if f.endswith(\".png\")])\n",
    "\n",
    "# Process images\n",
    "image_embeddings = [get_image_embedding(os.path.join(image_dir, file)) for file in image_filenames]\n",
    "image_embeddings = np.array(image_embeddings, dtype=\"float32\")\n",
    "image_embeddings /= np.linalg.norm(image_embeddings, axis=1, keepdims=True)  # Normalize\n",
    "\n",
    "# Create FAISS index\n",
    "image_index = faiss.IndexFlatL2(image_embeddings.shape[1])\n",
    "image_index.add(image_embeddings)\n",
    "\n",
    "print(\"✅ Stored flowchart image embeddings in FAISS database.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31fcba5a-5490-47e1-9685-7e66bd084120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "import re\n",
    "import fitz  \n",
    "\n",
    "def extract_text_from_image(image_path):\n",
    "    \"\"\"Extract text from a given flowchart image using OCR.\"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    extracted_text = pytesseract.image_to_string(image)\n",
    "    return extracted_text.strip()\n",
    "\n",
    "def get_text_embedding(text):\n",
    "    \"\"\"Generate an embedding for extracted text using Sentence-BERT.\"\"\"\n",
    "    return text_model.encode(text)\n",
    "\n",
    "# def extract_text_from_user_pdf(pdf_path):\n",
    "#     \"\"\"Extract text from a user-uploaded PDF file.\"\"\"\n",
    "#     text = \"\"\n",
    "#     with fitz.open(pdf_path) as doc:\n",
    "#         for page in doc:\n",
    "#             text += page.get_text(\"text\") + \"\\n\"\n",
    "#     return text.strip()\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2285b3dc-6fcf-4600-9620-b7540d81ec4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Retrieval function is ready.\n"
     ]
    }
   ],
   "source": [
    "def get_query_image_embedding(image_path):\n",
    "    \"\"\"Generate a normalized embedding for a query image using CLIP.\"\"\"\n",
    "    image_tensor = preprocess_image(image_path).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embedding = clip_model.encode_image(image_tensor).cpu().numpy()\n",
    "    \n",
    "    return embedding.flatten() / np.linalg.norm(embedding)  # Normalize\n",
    "\n",
    "\n",
    "def retrieve_relevant_data(query, flowchart_img=None, top_k=2):\n",
    "    \"\"\"Retrieve the most relevant documents & images for the query using FAISS.\"\"\"\n",
    "\n",
    "    # Convert query to text embedding\n",
    "    query_embedding = text_model.encode(query).reshape(1, -1)\n",
    "    query_embedding /= np.linalg.norm(query_embedding)  # Normalize\n",
    "\n",
    "    # Search FAISS text database (Retrieve relevant PDFs)\n",
    "    _, text_results = text_index.search(query_embedding, top_k)\n",
    "    retrieved_pdfs = [pdf_filenames[idx] for idx in text_results[0]]\n",
    "\n",
    "    # Process flowchart image if provided\n",
    "    if flowchart_img:\n",
    "        extracted_text = extract_text_from_image(flowchart_img)\n",
    "        text_embedding = get_text_embedding(extracted_text)\n",
    "\n",
    "        # Use extracted text to retrieve relevant documents\n",
    "        _, text_results = text_index.search(text_embedding.reshape(1, -1), top_k)\n",
    "        retrieved_pdfs = [pdf_filenames[idx] for idx in text_results[0]]\n",
    "\n",
    "        # Generate CLIP image embedding as well\n",
    "        image_query_embedding = get_query_image_embedding(flowchart_img)\n",
    "    else:\n",
    "        # Convert text query into CLIP text embedding\n",
    "        text_tokenized = clip.tokenize([query]).to(device)\n",
    "        with torch.no_grad():\n",
    "            image_query_embedding = clip_model.encode_text(text_tokenized).cpu().numpy()\n",
    "\n",
    "    image_query_embedding = image_query_embedding.reshape(1, -1)\n",
    "    image_query_embedding /= np.linalg.norm(image_query_embedding)\n",
    "\n",
    "    # Search FAISS image database (Retrieve relevant flowcharts)\n",
    "    _, image_results = image_index.search(image_query_embedding, top_k)\n",
    "    retrieved_images = [image_filenames[idx] for idx in image_results[0]]\n",
    "\n",
    "    return retrieved_pdfs, retrieved_images\n",
    "\n",
    "print(\"✅ Retrieval function is ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd22c25b-b1a2-459b-926b-60adfc08d7ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3c68e0c2a254687be4fd8fbc39d1545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration\n",
    "\n",
    "# Load Qwen model\n",
    "qwen_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-3B-Instruct\", \n",
    "    torch_dtype=torch.bfloat16,  \n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "qwen_processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "031ac69c-2195-4f38-86df-414c6e47d450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def debug_query_qwen_with_rag(query, flowchart_img=None, top_k=2):\n",
    "#     \"\"\"DEBUG MODE - Retrieve relevant flowchart data (text & images) and query Qwen for an AI-generated response.\"\"\"\n",
    "    \n",
    "#     # Retrieve relevant PDFs (text) & Flowcharts (images)\n",
    "#     retrieved_pdfs, retrieved_images = retrieve_relevant_data(query, flowchart_img, top_k)\n",
    "    \n",
    "#     # Extract text from retrieved PDFs\n",
    "#     context = \"\\n\".join([pdf_texts[pdf] for pdf in retrieved_pdfs])\n",
    "\n",
    "#     # Use retrieved flowchart image if `flowchart_img` is None\n",
    "#     image_path = flowchart_img if flowchart_img else os.path.join(image_dir, retrieved_images[0])\n",
    "#     image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "#     # Define the user message (Injecting retrieved context)\n",
    "#     messages = [\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": [\n",
    "#                 {\"type\": \"image\", \"image\": image},\n",
    "#                 {\"type\": \"text\", \"text\": f\"Attached is document providing context. Based on the provided image and textual information, please analyze the content and generate a response that accurately addresses the user's inquiry.\\n\\nContext:\\n{context}\\n\\nQuery: {query}\"},\n",
    "#             ],\n",
    "#         }\n",
    "#     ]\n",
    "    \n",
    "#     # Format input for Qwen\n",
    "#     text = qwen_processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "#     inputs = qwen_processor(\n",
    "#         text=[text],\n",
    "#         images=[image],\n",
    "#         padding=True,\n",
    "#         return_tensors=\"pt\",\n",
    "#     ).to(qwen_model.device)\n",
    "    \n",
    "#     # Generate response\n",
    "#     with torch.no_grad():\n",
    "#         output_ids = qwen_model.generate(**inputs, max_new_tokens=512)\n",
    "    \n",
    "#     # Decode response\n",
    "#     response_text = \"========\\n\\n\".join(qwen_processor.batch_decode(output_ids, skip_special_tokens=False))\n",
    "    \n",
    "#     return response_text, retrieved_pdfs, retrieved_images\n",
    "\n",
    "# print(\"✅ DEBUG Qwen RAG system is ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cee3aa1f-4093-4a85-bf0b-08e159d6f64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Multi-Image Qwen RAG system is ready.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from PIL import Image\n",
    "\n",
    "def query_qwen_with_rag(query, flowchart_imgs=None, top_k=2):\n",
    "    \"\"\"Retrieve relevant flowchart data (text & images) and query Qwen for an AI-generated response.\"\"\"\n",
    "\n",
    "    retrieved_pdfs = []\n",
    "    retrieved_images = []\n",
    "    extracted_texts = []\n",
    "    image_objects = []\n",
    "    image_embeddings = []\n",
    "\n",
    "    # Process each image if multiple images are provided\n",
    "    if flowchart_imgs:\n",
    "        for img_path in flowchart_imgs:\n",
    "            extracted_text = extract_text_from_image(img_path)  # OCR text extraction\n",
    "            extracted_texts.append(extracted_text)\n",
    "\n",
    "            # Generate text embedding from extracted OCR text\n",
    "            text_embedding = get_text_embedding(extracted_text)\n",
    "            _, text_results = text_index.search(text_embedding.reshape(1, -1), top_k)\n",
    "            retrieved_pdfs.extend([pdf_filenames[idx] for idx in text_results[0]])\n",
    "\n",
    "            # Generate CLIP image embeddings\n",
    "            image_embedding = get_query_image_embedding(img_path)\n",
    "            image_embeddings.append(image_embedding)\n",
    "\n",
    "            # Load image for Qwen\n",
    "            image_objects.append(Image.open(img_path).convert(\"RGB\"))\n",
    "\n",
    "    else:\n",
    "        extracted_texts.append(\"\")\n",
    "\n",
    "    # Aggregate extracted texts from multiple images\n",
    "    extracted_text_context = \"\\n\".join(extracted_texts)\n",
    "\n",
    "    # Aggregate multiple image embeddings (mean vector)\n",
    "    if image_embeddings:\n",
    "        aggregated_image_embedding = np.mean(image_embeddings, axis=0)\n",
    "        aggregated_image_embedding /= np.linalg.norm(aggregated_image_embedding)  # Normalize\n",
    "\n",
    "        # Retrieve relevant flowcharts from FAISS\n",
    "        _, image_results = image_index.search(aggregated_image_embedding.reshape(1, -1), top_k)\n",
    "        retrieved_images = [image_filenames[idx] for idx in image_results[0]]\n",
    "\n",
    "    # Retrieve documents based on text query\n",
    "    query_embedding = text_model.encode(query).reshape(1, -1)\n",
    "    query_embedding /= np.linalg.norm(query_embedding)\n",
    "\n",
    "    _, text_results = text_index.search(query_embedding, top_k)\n",
    "    retrieved_pdfs.extend([pdf_filenames[idx] for idx in text_results[0]])\n",
    "\n",
    "    # Deduplicate retrieved PDFs\n",
    "    retrieved_pdfs = list(set(retrieved_pdfs))\n",
    "\n",
    "    # Define the user message (Injecting retrieved context)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                *[{\"type\": \"image\", \"image\": img} for img in image_objects],  # Pass all images\n",
    "                {\"type\": \"text\", \"text\": f\"Given the attached flowcharts, analyze their content.\\n\\nExtracted Flowchart Texts:\\n{extracted_text_context}\\n\\nContext from Documents:\\n{retrieved_pdfs}\\n\\nQuery: {query}\"},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Format input for Qwen\n",
    "    text = qwen_processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = qwen_processor(\n",
    "        text=[text],\n",
    "        images=image_objects,  # Pass multiple images\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(qwen_model.device)\n",
    "    \n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        output_ids = qwen_model.generate(**inputs, max_new_tokens=800)\n",
    "    \n",
    "    # Decode response\n",
    "    response_text = qwen_processor.batch_decode(output_ids, skip_special_tokens=False)[0]\n",
    "\n",
    "    # Extract Assistant's Response Only\n",
    "    match = re.search(r\"assistant\\s*\\n(.*)\", response_text, re.DOTALL)\n",
    "    cleaned_response = match.group(1).strip() if match else response_text.strip()\n",
    "\n",
    "    return cleaned_response, retrieved_pdfs, retrieved_images\n",
    "\n",
    "print(\"✅ Multi-Image Qwen RAG system is ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ae4f055-8f6e-48fb-b117-c4ae3962c608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🤖 Qwen's Response (With RAG):\n",
      "Certainly! Let's compare the two flowcharts in detail:\n",
      "\n",
      "### First Flowchart:\n",
      "**Title:** Incident Management\n",
      "\n",
      "**Flowchart Text:**\n",
      "```\n",
      "Incident Management\n",
      "\n",
      "Start\n",
      "\n",
      "User Reports Issue\n",
      "\n",
      "Classify Incident\n",
      "\n",
      "High Impact?\n",
      "- No -> Assign to Support Team\n",
      "- Yes -> Escalate to Higher Support\n",
      "\n",
      "Issue Resolved?\n",
      "- Yes -> Close Ticket\n",
      "- No -> Resolve Issue\n",
      "```\n",
      "\n",
      "### Second Flowchart:\n",
      "**Title:** Incident Management\n",
      "\n",
      "**Flowchart Text:**\n",
      "```\n",
      "Incident Management\n",
      "\n",
      "User Reports Issue\n",
      "\n",
      "Initial Triage\n",
      "\n",
      "Classify Incident\n",
      "\n",
      "Automated Resolution?\n",
      "- No -> High Impact?\n",
      "  - Yes -> Critical?\n",
      "    - Yes -> Escalate to Higher Support\n",
      "    - No -> Assign to Support Team\n",
      "  - No -> Resolve Issue\n",
      "- Yes -> Follow-up Needed?\n",
      "\n",
      "Issue Resolved?\n",
      "- Yes -> Close Ticket\n",
      "- No -> Resolve Issue\n",
      "```\n",
      "\n",
      "### Differences Between the Two Flowcharts:\n",
      "\n",
      "1. **Initial Triage:**\n",
      "   - The first flowchart does not include an initial triage step.\n",
      "   - The second flowchart includes an \"Initial Triage\" step before classifying the incident.\n",
      "\n",
      "2. **Automated Resolution:**\n",
      "   - The first flowchart does not have an \"Automated Resolution?\" step.\n",
      "   - The second flowchart has an \"Automated Resolution?\" step that determines whether the issue can be resolved automatically or requires manual intervention.\n",
      "\n",
      "3. **High Impact Decision:**\n",
      "   - The first flowchart directly assigns high-impact issues to support teams without further classification.\n",
      "   - The second flowchart classifies high-impact issues into critical and non-critical categories before deciding whether to escalate them.\n",
      "\n",
      "4. **Escalation:**\n",
      "   - The first flowchart escalates high-impact issues directly to higher support.\n",
      "   - The second flowchart escalates high-impact issues only if they are critical.\n",
      "\n",
      "5. **Follow-up Needed:**\n",
      "   - The first flowchart does not have a \"Follow-up Needed?\" step.\n",
      "   - The second flowchart includes a \"Follow-up Needed?\" step after resolving an issue, which determines whether follow-up is required.\n",
      "\n",
      "6. **Resolution Process:**\n",
      "   - The first flowchart resolves issues manually and then closes tickets.\n",
      "   - The second flowchart allows for automated resolution and manual resolution, with a focus on follow-up after resolution.\n",
      "\n",
      "These differences provide more detailed steps and decision points in the incident management process, making it more structured and potentially more efficient.<|im_end|>\n",
      "🔍 Retrieved Documents: ['6_Incident_Management.pdf', '5.pdf', '18_Medication_Prescription.pdf', '10_Help_Desk_Ticketing.pdf']\n",
      "🖼️ Retrieved Flowcharts: ['6_incident_management.png', '3.png']\n",
      "⏳ Execution Time: 10.09 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Define the test query and multiple flowchart images\n",
    "# user_query = \"Provided here is an incident management flowchart. The first one is in the databse, the second one is an improved one I'm working on. How many nodes are on each flowcharts? List it all out\"\n",
    "user_query = \"Can you compare these flowcharts in detail? What are the difference between the first and the second one. I'm working on the second one to give more detail.\"\n",
    "flowchart_imgs = [    \n",
    "    \"business_flowcharts/flowcharts/6_incident_management.png\",\n",
    "    \"6_incident_management_v2.png\",\n",
    "]\n",
    "# user_pdf = [\n",
    "#     \"6_Incident_Management_V2.pdf\n",
    "# ]\n",
    "\n",
    "# \"business_flowcharts/flowcharts/6_incident_management.png\",\n",
    "\n",
    "# Measure execution time\n",
    "start_time = time.time()\n",
    "\n",
    "# Query Qwen with RAG\n",
    "qwen_response_rag, retrieved_pdfs, retrieved_images = query_qwen_with_rag(user_query, flowchart_imgs)\n",
    "\n",
    "# Calculate total time taken\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Print results\n",
    "print(f\"\\n🤖 Qwen's Response (With RAG):\\n{qwen_response_rag}\")\n",
    "print(f\"🔍 Retrieved Documents: {retrieved_pdfs}\")\n",
    "print(f\"🖼️ Retrieved Flowcharts: {retrieved_images}\")\n",
    "# Add user uploaded images here\n",
    "# Add user uploaded files here\n",
    "print(f\"⏳ Execution Time: {execution_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f206f439-7b0a-4ae7-afc1-bdfdfbbdba64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🤖 Qwen's Response (With RAG):\n",
      "Certainly! Let's compare the two flowcharts in detail:\n",
      "\n",
      "### Original Cybersecurity Incident Response Flowchart\n",
      "\n",
      "1. **Start**\n",
      "2. **Detect Security Threat**\n",
      "3. **Analyze Threat**\n",
      "4. **Critical Threat?**\n",
      "   - If **Yes**: Deploy Security Patch\n",
      "   - If **No**: Apply Mitigation\n",
      "5. **Monitor System**\n",
      "6. **Incident Resolved**\n",
      "\n",
      "### Upgraded Cybersecurity Incident Response Flowchart\n",
      "\n",
      "1. **Start**\n",
      "2. **Detect Security Threat**\n",
      "3. **Classify Threat**\n",
      "4. **Analyze Threat Impact**\n",
      "5. **Critical Threat?**\n",
      "   - If **Yes**: Escalate to Security Team\n",
      "   - If **No**: Apply Mitigation Measures\n",
      "6. **Deploy Security Patch**\n",
      "7. **Monitor System for Anomalies**\n",
      "8. **Post-Incident Review & Reporting**\n",
      "9. **Incident Resolved**\n",
      "\n",
      "### Differences Between the Two Flowcharts\n",
      "\n",
      "1. **Classification of Threat**:\n",
      "   - **Original Flowchart**: No specific classification step.\n",
      "   - **Upgraded Flowchart**: Added a \"Classify Threat\" step.\n",
      "\n",
      "2. **Impact Analysis**:\n",
      "   - **Original Flowchart**: No impact analysis step.\n",
      "   - **Upgraded Flowchart**: Added an \"Analyze Threat Impact\" step.\n",
      "\n",
      "3. **Mitigation Measures**:\n",
      "   - **Original Flowchart**: Directly applies mitigation measures if no critical threat is detected.\n",
      "   - **Upgraded Flowchart**: Applies mitigation measures only if there is no critical threat, and then escalates to the security team if necessary.\n",
      "\n",
      "4. **Security Patch Deployment**:\n",
      "   - **Original Flowchart**: Directly deploys the security patch after applying mitigation measures.\n",
      "   - **Upgraded Flowchart**: Deploys the security patch after applying mitigation measures and monitoring system for anomalies.\n",
      "\n",
      "5. **Monitoring System**:\n",
      "   - **Original Flowchart**: Monitors the system directly after applying mitigation measures.\n",
      "   - **Upgraded Flowchart**: Monitors the system for anomalies after deploying the security patch.\n",
      "\n",
      "6. **Post-Incident Review & Reporting**:\n",
      "   - **Original Flowchart**: No post-incident review or reporting step.\n",
      "   - **Upgraded Flowchart**: Added a \"Post-Incident Review & Reporting\" step.\n",
      "\n",
      "7. **Escalation to Security Team**:\n",
      "   - **Original Flowchart**: No escalation to security team step.\n",
      "   - **Upgraded Flowchart**: Added an \"Escalate to Security Team\" step when a critical threat is detected.\n",
      "\n",
      "These upgrades provide a more detailed and structured approach to handling cybersecurity incidents, ensuring that all potential threats are thoroughly analyzed and mitigated before any security patches are deployed.<|im_end|>\n",
      "🔍 Retrieved Documents: ['6_Incident_Management.pdf', '9_Cybersecurity_Incident_Response.pdf', '18_Medication_Prescription.pdf', '5.pdf']\n",
      "🖼️ Retrieved Flowcharts: ['9_cybersecurity_incident_response.png', '6_incident_management.png']\n",
      "⏳ Execution Time: 10.38 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Define the test query and multiple flowchart images\n",
    "# user_query = \"Provided here is an incident management flowchart. The first one is in the databse, the second one is an improved one I'm working on. How many nodes are on each flowcharts? List it all out\"\n",
    "user_query = \"Can you compare these flowcharts in detail? What are the difference between the first and the second one. I'm working on the second one to give more detail.\"\n",
    "flowchart_imgs = [    \n",
    "    \"business_flowcharts/flowcharts/9_cybersecurity_incident_response.png\",\n",
    "    \"9_cybersecurity_incident_response_v2.png\",\n",
    "]\n",
    "# user_pdf = [\n",
    "#     \"6_Incident_Management_V2.pdf\n",
    "# ]\n",
    "\n",
    "# \"business_flowcharts/flowcharts/6_incident_management.png\",\n",
    "\n",
    "# Measure execution time\n",
    "start_time = time.time()\n",
    "\n",
    "# Query Qwen with RAG\n",
    "qwen_response_rag, retrieved_pdfs, retrieved_images = query_qwen_with_rag(user_query, flowchart_imgs)\n",
    "\n",
    "# Calculate total time taken\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Print results\n",
    "print(f\"\\n🤖 Qwen's Response (With RAG):\\n{qwen_response_rag}\")\n",
    "print(f\"🔍 Retrieved Documents: {retrieved_pdfs}\")\n",
    "print(f\"🖼️ Retrieved Flowcharts: {retrieved_images}\")\n",
    "# Add user uploaded images here\n",
    "# Add user uploaded files here\n",
    "print(f\"⏳ Execution Time: {execution_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cbc62a-af1c-47bb-858d-d5f0b2c71c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def debug_flowchart_inputs(flowchart_imgs):\n",
    "#     \"\"\"Display and log flowchart images along with extracted OCR text.\"\"\"\n",
    "    \n",
    "#     fig, axes = plt.subplots(1, len(flowchart_imgs), figsize=(10 * len(flowchart_imgs), 5))\n",
    "    \n",
    "#     if len(flowchart_imgs) == 1:\n",
    "#         axes = [axes]  # Ensure iterable for single image\n",
    "    \n",
    "#     extracted_texts = []\n",
    "    \n",
    "#     for i, img_path in enumerate(flowchart_imgs):\n",
    "#         extracted_text = extract_text_from_image(img_path)\n",
    "#         extracted_texts.append(extracted_text)\n",
    "#         print(f\"\\n🔍 Extracted Text from {img_path}:\\n{extracted_text}\\n\")\n",
    "        \n",
    "#         img = Image.open(img_path)\n",
    "#         axes[i].imshow(img)\n",
    "#         axes[i].set_title(f\"Flowchart {i+1}\")\n",
    "#         axes[i].axis(\"off\")\n",
    "\n",
    "#     plt.show()\n",
    "    \n",
    "#     return extracted_texts\n",
    "\n",
    "# debug_flowchart_inputs(flowchart_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26fda206-ca13-4497-9313-462900efb1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test saved documents in FAISS database\n",
    "# def view_stored_pdfs():\n",
    "#     \"\"\"Display all stored PDFs and their extracted text.\"\"\"\n",
    "#     for filename, text in pdf_texts.items():\n",
    "#         print(f\"📄 PDF: {filename}\\n\")\n",
    "#         print(f\"Extracted Content:\\n{text[:1000]}\")  # Show first 1000 characters\n",
    "#         print(\"=\"*80)\n",
    "\n",
    "# view_stored_pdfs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "766448a9-e470-49d0-9eff-3adef9c37473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Test saved images in FAISS database\n",
    "\n",
    "# def view_stored_flowcharts():\n",
    "#     \"\"\"Display all stored flowchart images and their filenames.\"\"\"\n",
    "#     for filename in image_filenames:\n",
    "#         image_path = os.path.join(image_dir, filename)\n",
    "#         image = Image.open(image_path)\n",
    "        \n",
    "#         plt.figure(figsize=(5, 5))\n",
    "#         plt.imshow(image)\n",
    "#         plt.axis(\"off\")\n",
    "#         plt.title(f\"🖼️ Flowchart: {filename}\")\n",
    "#         plt.show()\n",
    "\n",
    "# view_stored_flowcharts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9e5fcdf-793f-41ba-a300-ff851d41b6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_pdf_embedding(index=11):\n",
    "#     \"\"\"Check stored text embedding by retrieving the closest match for a given PDF.\"\"\"\n",
    "#     query_embedding = pdf_embeddings[index].reshape(1, -1)\n",
    "#     _, retrieved_indices = text_index.search(query_embedding, 1)\n",
    "    \n",
    "#     original_pdf = pdf_filenames[index]\n",
    "#     matched_pdf = pdf_filenames[retrieved_indices[0][0]]\n",
    "    \n",
    "#     print(f\"📄 Original PDF: {original_pdf}\")\n",
    "#     print(f\"🔍 Closest Match: {matched_pdf}\")\n",
    "#     print(f\"Similarity Score: {np.dot(pdf_embeddings[index], pdf_embeddings[retrieved_indices[0][0]])}\")\n",
    "    \n",
    "# check_pdf_embedding()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b23561cd-0ea7-427f-8c03-41c278131caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def check_image_embedding(index=3):\n",
    "#     \"\"\"Check stored image embedding by retrieving the closest match for a given flowchart.\"\"\"\n",
    "#     query_embedding = image_embeddings[index].reshape(1, -1)\n",
    "#     _, retrieved_indices = image_index.search(query_embedding, 1)\n",
    "    \n",
    "#     original_image = image_filenames[index]\n",
    "#     matched_image = image_filenames[retrieved_indices[0][0]]\n",
    "    \n",
    "#     print(f\"🖼️ Original Flowchart: {original_image}\")\n",
    "#     print(f\"🔍 Closest Match: {matched_image}\")\n",
    "#     print(f\"Similarity Score: {np.dot(image_embeddings[index], image_embeddings[retrieved_indices[0][0]])}\")\n",
    "    \n",
    "#     # Show both images\n",
    "#     fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    \n",
    "#     ax[0].imshow(Image.open(os.path.join(image_dir, original_image)))\n",
    "#     ax[0].set_title(\"Original Image\")\n",
    "#     ax[0].axis(\"off\")\n",
    "    \n",
    "#     ax[1].imshow(Image.open(os.path.join(image_dir, matched_image)))\n",
    "#     ax[1].set_title(\"Closest Match\")\n",
    "#     ax[1].axis(\"off\")\n",
    "    \n",
    "#     plt.show()\n",
    "\n",
    "# check_image_embedding()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ef5c1cd-0f45-4618-975d-6d379d74c79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check if all image embeddings are unique\n",
    "# unique_embeddings = np.unique(image_embeddings, axis=0)\n",
    "\n",
    "# if unique_embeddings.shape[0] == 1:\n",
    "#     print(\"⚠️ WARNING: All image embeddings are identical! FAISS cannot differentiate them.\")\n",
    "# else:\n",
    "#     print(f\"✅ FAISS has {unique_embeddings.shape[0]} unique image embeddings.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "019b9bdf-1990-476b-9466-49693196db84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print shapes of stored image embeddings\n",
    "# print(f\"Stored Image Embeddings Shape: {image_embeddings.shape}\")\n",
    "\n",
    "# # Generate a query embedding for comparison\n",
    "# query_embedding = get_query_image_embedding(os.path.join(image_dir, image_filenames[0]))  # Use any image as query\n",
    "# print(f\"Query Image Embedding Shape: {query_embedding.shape}\")\n",
    "\n",
    "# # Print first stored embedding vs query embedding\n",
    "# print(f\"\\nFirst Stored Embedding:\\n{image_embeddings[0][:10]}\")  # Print first 10 values\n",
    "# print(f\"\\nQuery Embedding:\\n{query_embedding[:10]}\")  # Print first 10 values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebf60dc3-dc92-4790-affa-50e372f30d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def debug_faiss_retrieval(index=3):\n",
    "#     \"\"\"Check if FAISS is properly differentiating images.\"\"\"\n",
    "#     query_embedding = image_embeddings[index].reshape(1, -1)  # Use stored image for retrieval test\n",
    "#     _, retrieved_indices = image_index.search(query_embedding, 5)  # Top 5 results\n",
    "\n",
    "#     print(f\"🖼️ Original Flowchart: {image_filenames[index]}\")\n",
    "#     print(f\"\\n🔍 Closest Matches:\")\n",
    "#     for rank, idx in enumerate(retrieved_indices[0]):\n",
    "#         matched_image = image_filenames[idx]\n",
    "#         similarity_score = np.dot(image_embeddings[index], image_embeddings[idx])  # Cosine similarity\n",
    "#         print(f\"{rank + 1}. {matched_image} (Score: {similarity_score:.6f})\")\n",
    "\n",
    "# debug_faiss_retrieval()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
