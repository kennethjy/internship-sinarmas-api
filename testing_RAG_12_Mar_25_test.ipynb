{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d6dcaa-6e8b-4df4-9642-c389c37af473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install faiss-cpu sentence-transformers PyMuPDF torchvision\n",
    "# !pip install git+https://github.com/openai/CLIP.git \n",
    "# Note that CLIP can't be downloaded directly. Requires a git clone of the link. \n",
    "# It is a multi-modal AI model developed by OpenAI that can understand and relate text and images. \n",
    "# It allows us to encode images and text into a shared embedding space, making it useful for image retrieval, classification, and search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c45815e3-2d4e-432e-a18d-099e6e919679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "import fitz  \n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration\n",
    "\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "711ce78c-a5c3-4564-b835-e4b32ee5fd57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GPU Memory: 24576 MB\n",
      "Used GPU Memory: 9812 MB\n",
      "Free GPU Memory: 14334 MB\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def check_cuda_memory():\n",
    "    try:\n",
    "        output = subprocess.check_output([\"nvidia-smi\", \"--query-gpu=memory.total,memory.used,memory.free\", \"--format=csv,nounits,noheader\"])\n",
    "        total, used, free = map(int, output.decode(\"utf-8\").strip().split(\"\\n\")[0].split(\", \"))\n",
    "        print(f\"Total GPU Memory: {total} MB\")\n",
    "        print(f\"Used GPU Memory: {used} MB\")\n",
    "        print(f\"Free GPU Memory: {free} MB\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "check_cuda_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a0a007f-be43-40b7-8cae-0281bed0a839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49e67b79b3df4f85ac222bdce5778f98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Qwen2.5-VL-3B model loaded successfully!\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# Free up memory before running Qwen\n",
    "torch.cuda.empty_cache()  \n",
    "gc.collect()\n",
    "\n",
    "# Load text embedding model\n",
    "text_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Load CLIP model for image embeddings\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "# clip_model, preprocess = clip.load(\"ViT-B/32\", device=\"cpu\")\n",
    "\n",
    "# Load Qwen2.5-VL-3B model & processor\n",
    "qwen_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-3B-Instruct\", \n",
    "    torch_dtype=torch.bfloat16,  # bfloat16 is more memory-efficient\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\"  # Auto-manages CPU/GPU offloading\n",
    ")\n",
    "\n",
    "qwen_processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\")\n",
    "\n",
    "print(\"âœ… Qwen2.5-VL-3B model loaded successfully!\")\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "deebed08-7271-4252-998a-8a945099d250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GPU Memory: 24576 MB\n",
      "Used GPU Memory: 18010 MB\n",
      "Free GPU Memory: 6137 MB\n"
     ]
    }
   ],
   "source": [
    "import subprocess \n",
    "\n",
    "def check_cuda_memory():\n",
    "    try:\n",
    "        output = subprocess.check_output([\"nvidia-smi\", \"--query-gpu=memory.total,memory.used,memory.free\", \"--format=csv,nounits,noheader\"])\n",
    "        total, used, free = map(int, output.decode(\"utf-8\").strip().split(\"\\n\")[0].split(\", \"))\n",
    "        print(f\"Total GPU Memory: {total} MB\")\n",
    "        print(f\"Used GPU Memory: {used} MB\")\n",
    "        print(f\"Free GPU Memory: {free} MB\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "check_cuda_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e51ee44-c3f1-43a8-9dfc-6da7e08dc60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a given PDF file.\"\"\"\n",
    "    text = \"\"\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        for page in doc:\n",
    "            text += page.get_text(\"text\") + \"\\n\"\n",
    "    return text.strip()\n",
    "\n",
    "# Define directory paths\n",
    "base_dir = \"business_flowcharts\"\n",
    "pdf_dir = os.path.join(base_dir, \"documents\")\n",
    "image_dir = os.path.join(base_dir, \"flowcharts\")\n",
    "\n",
    "# Read all PDFs and extract text\n",
    "pdf_texts = {}\n",
    "for file in sorted(os.listdir(pdf_dir)):  \n",
    "    if file.endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(pdf_dir, file)\n",
    "        text = extract_text_from_pdf(pdf_path)\n",
    "        pdf_texts[file] = text\n",
    "        print(f\"Extracted text from {file}\")\n",
    "\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087b7954-7e25-4442-accf-fcc03a205649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for p in pdf_texts.values():\n",
    "#     print(p)\n",
    "#     print(\"===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5778a2e0-bef6-41dc-81e4-cbeaad613b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert extracted PDF texts to embeddings\n",
    "pdf_embeddings = []\n",
    "pdf_filenames = list(pdf_texts.keys())\n",
    "\n",
    "for filename in pdf_filenames:\n",
    "    embedding = text_model.encode(pdf_texts[filename])\n",
    "    pdf_embeddings.append(embedding)\n",
    "\n",
    "# Convert list to NumPy array for FAISS\n",
    "pdf_embeddings = np.array(pdf_embeddings, dtype=\"float32\")\n",
    "\n",
    "# Create FAISS index for text embeddings\n",
    "text_index = faiss.IndexFlatL2(pdf_embeddings.shape[1])\n",
    "text_index.add(pdf_embeddings)\n",
    "\n",
    "print(\"Stored document text embeddings in FAISS database.\")\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29ad44b-3d4d-47c6-953a-6db788949f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_embedding(image_path):\n",
    "    \"\"\"Generate an embedding for a flowchart image using CLIP.\"\"\"\n",
    "    image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        embedding = clip_model.encode_image(image).cpu().numpy()\n",
    "    return embedding.flatten()\n",
    "\n",
    "# Process and store embeddings for flowchart images\n",
    "image_embeddings = []\n",
    "image_filenames = []\n",
    "\n",
    "for file in sorted(os.listdir(image_dir)):  # Ensure order matches PDFs\n",
    "    if file.endswith(\".png\"):\n",
    "        image_path = os.path.join(image_dir, file)\n",
    "        embedding = get_image_embedding(image_path)\n",
    "        image_embeddings.append(embedding)\n",
    "        image_filenames.append(file)\n",
    "\n",
    "# Convert list to NumPy array for FAISS\n",
    "image_embeddings = np.array(image_embeddings, dtype=\"float32\")\n",
    "\n",
    "# Create FAISS index for image embeddings\n",
    "image_index = faiss.IndexFlatL2(image_embeddings.shape[1])\n",
    "image_index.add(image_embeddings)\n",
    "\n",
    "print(\"Stored flowchart image embeddings in FAISS database.\")\n",
    "\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f77f6d38-0abe-48f4-a448-4782acc8b1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# No RAG\n",
    "# Function to query Qwen with both text and an image\n",
    "def query_qwen_with_image(image_path, query_text):\n",
    "    \"\"\"Send an image and text query to Qwen2.5-VL-3B.\"\"\"\n",
    "    \n",
    "    # Load the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # Define the user message\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": image},\n",
    "                {\"type\": \"text\", \"text\": query_text},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Format input for Qwen\n",
    "    text = qwen_processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = qwen_processor(\n",
    "        text=[text],\n",
    "        images=[image],  # Provide the image input\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(qwen_model.device)\n",
    "\n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        output_ids = qwen_model.generate(**inputs, max_new_tokens=512)\n",
    "\n",
    "    # Decode response\n",
    "    response_text = qwen_processor.batch_decode(output_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    return response_text\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb51f392-80bc-40b0-bab6-f0613aa993a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Qwen's Response:\n",
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "In this Customer Support Ticket flowchart, what happens If further assistance is required?\n",
      "assistant\n",
      "If further assistance is required, the process moves to \"Escalate to Senior Support.\"\n"
     ]
    }
   ],
   "source": [
    "# NO RAG\n",
    "# query_text = \"In this order processing flowchart, who prepares and packs the order upon a successful payment?\"\n",
    "# image_path = \"business_flowcharts/flowcharts/4.png\"  # Flowchart image here\n",
    "query_text = \"In this Customer Support Ticket flowchart, what happens If further assistance is required?\"\n",
    "image_path = \"business_flowcharts/flowcharts/1.png\"\n",
    "\n",
    "qwen_response = query_qwen_with_image(image_path, query_text)\n",
    "print(f\"ðŸ¤– Qwen's Response:\\n{qwen_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709b407b-cf75-4f07-9ba2-299101492a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With RAG\n",
    "# Function to Retrieve Relevant PDFs & Flowcharts\n",
    "def retrieve_relevant_data(query, top_k=2):\n",
    "    \"\"\"Retrieve the most relevant documents & images for the query using FAISS.\"\"\"\n",
    "    \n",
    "    # Convert query to text embedding\n",
    "    query_embedding = text_model.encode(query).reshape(1, -1)\n",
    "    \n",
    "    # Search FAISS text database (Retrieve relevant PDFs)\n",
    "    _, text_results = text_index.search(query_embedding, top_k)\n",
    "    retrieved_pdfs = [pdf_filenames[idx] for idx in text_results[0]]\n",
    "    \n",
    "    # Convert query to image embedding using CLIP\n",
    "    text_tokenized = clip.tokenize([query]).to(device)\n",
    "    with torch.no_grad():\n",
    "        image_query_embedding = clip_model.encode_text(text_tokenized).cpu().numpy()\n",
    "    \n",
    "    # Search FAISS image database (Retrieve relevant flowcharts)\n",
    "    _, image_results = image_index.search(image_query_embedding, top_k)\n",
    "    retrieved_images = [image_filenames[idx] for idx in image_results[0]]\n",
    "\n",
    "    return retrieved_pdfs, retrieved_images\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7051d7-fb87-4396-8dbc-da87848708b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Query Qwen with RAG (Flowchart Context)\n",
    "def query_qwen_with_rag(query, flowchart_img, top_k=2):\n",
    "    \"\"\"Retrieve relevant flowchart data (text & images) and query Qwen for an AI-generated response.\"\"\"\n",
    "    \n",
    "    # Retrieve relevant PDFs (text) & Flowcharts (images)\n",
    "    retrieved_pdfs, retrieved_images = retrieve_relevant_data(query, top_k)\n",
    "    \n",
    "    # Extract text from retrieved PDFs\n",
    "    context = \"\\n\".join([pdf_texts[pdf] for pdf in retrieved_pdfs])\n",
    "    \n",
    "    # Load the provided flowchart image\n",
    "    image = Image.open(flowchart_img).convert(\"RGB\")\n",
    "    \n",
    "    # Define the user message (Injecting retrieved context)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": image},  # Provide the provided image\n",
    "                {\"type\": \"text\", \"text\": f\"Attached is document providing context. Based on the provided image and textual information, please analyze the content and generate a response that accurately addresses the user's inquiry. The document contains relevant details that should be considered in forming a well-informed answer. Ensure that your response integrates both visual and textual elements for a comprehensive analysis.\\n\\nContext:\\n{context}\\n\\nQuery: {query}\"},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Format input for Qwen\n",
    "    text = qwen_processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = qwen_processor(\n",
    "        text=[text],\n",
    "        images=[image],  # Provide the image input\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(qwen_model.device)\n",
    "    \n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        output_ids = qwen_model.generate(**inputs, max_new_tokens=512)\n",
    "    \n",
    "    # Decode response\n",
    "    response_text = \"========\\n\\n\".join(qwen_processor.batch_decode(output_ids, skip_special_tokens=False))\n",
    "    \n",
    "    return response_text, retrieved_pdfs, retrieved_images\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2987ff4-0176-41a1-bd10-ed5febf88f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define query and flowchart image\n",
    "# query = \"Can you see the flowchart i provided here about customer support? What if further assistance is needed?\"\n",
    "# flowchart_img = \"business_flowcharts/flowcharts/1.png\"\n",
    "\n",
    "# # Query Qwen with RAG\n",
    "# qwen_response_rag, retrieved_pdfs, retrieved_images = query_qwen_with_rag(query, flowchart_img)\n",
    "\n",
    "# # Print results\n",
    "# print(f\"\\nðŸ¤– Qwen's Response (With RAG):\\n{qwen_response_rag}\")\n",
    "# print(f\"ðŸ” Retrieved Documents: {retrieved_pdfs}\")\n",
    "# print(f\"ðŸ–¼ï¸ Retrieved Flowcharts: {retrieved_images}\")\n",
    "\n",
    "ðŸ” Retrieved Documents: ['23_Academic_Research.pdf', '21_Student_Enrollment.pdf'] \n",
    "â³ Execution Time: 2.24 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4183f034-dc03-41f9-8aa5-8cb5bdc6d5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# user_query = \"In this Employee Onboarding flowchart, what will the employees learn during orientation? Is additional training provided should it be needed?\"\n",
    "# user_query = \"What can you tell me about this flowchart? I want all the details.\"\n",
    "# user_query = \"In this order processing flowchart, who prepares and packs the order upon a successful payment?\"\n",
    "# flowchart_img = \"business_flowcharts/flowcharts/4.png\"\n",
    "user_query = \"What can you tell me about this flowchart on academic research? Can you explain each step in great detail?\"\n",
    "flowchart_img = \"business_flowcharts/flowcharts/23_academic_research.png\"\n",
    "\n",
    "# Measure execution time\n",
    "start_time = time.time()\n",
    "\n",
    "# Query Qwen with RAG\n",
    "qwen_response_rag, retrieved_pdfs, retrieved_images = query_qwen_with_rag(user_query, flowchart_img)\n",
    "\n",
    "# Calculate total time taken\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nðŸ¤– Qwen's Response (With RAG):\\n{qwen_response_rag}\")\n",
    "print(f\"ðŸ” Retrieved Documents: {retrieved_pdfs}\")\n",
    "print(f\"ðŸ–¼ï¸ Retrieved Flowcharts: {retrieved_images}\")\n",
    "print(f\"â³ Execution Time: {execution_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9b6a80-63ca-4b2f-b887-94c74e0d24e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
