{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9abb87f1-8a0b-4c70-bed8-bd88b931f888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install qwen_vl_utils\n",
    "# !pip install sentence-transformers faiss-cpu transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b8e97a5-dcc1-4210-952a-3945ed6eaeb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GPU Memory: 24576 MB\n",
      "Used GPU Memory: 9060 MB\n",
      "Free GPU Memory: 15086 MB\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def check_cuda_memory():\n",
    "    try:\n",
    "        output = subprocess.check_output([\"nvidia-smi\", \"--query-gpu=memory.total,memory.used,memory.free\", \"--format=csv,nounits,noheader\"])\n",
    "        total, used, free = map(int, output.decode(\"utf-8\").strip().split(\"\\n\")[0].split(\", \"))\n",
    "        print(f\"Total GPU Memory: {total} MB\")\n",
    "        print(f\"Used GPU Memory: {used} MB\")\n",
    "        print(f\"Free GPU Memory: {free} MB\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "check_cuda_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc32e57a-5d21-49e2-9077-0920ac30f800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7 documents about oranges.\n"
     ]
    }
   ],
   "source": [
    "# Testing with local dataset of oranges. Why not banana? Later ;3\n",
    "documents = [\n",
    "    \"Oranges are citrus fruits rich in vitamin C.\",\n",
    "    \"An orange tree can live for 20 to 30 years, but some can reach 100 years.\",\n",
    "    \"Orange juice is a popular breakfast drink worldwide.\",\n",
    "    \"Oranges contain antioxidants that boost the immune system.\",\n",
    "    \"Brazil is no longer the world's largest producer of oranges. It is China now, producing up to 60% of the world's orange population.\",\n",
    "    \"The peel of an orange can be used for essential oils and flavoring.\",\n",
    "    \"Oranges originated in Southeast Asia thousands of years ago.\"\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents about oranges.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "972f3fb2-5e83-4df6-9e4a-1a62cfe11b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 7 embeddings.\n"
     ]
    }
   ],
   "source": [
    "# Convert documents to embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "document_embeddings = [embed_model.encode(doc) for doc in documents]\n",
    "\n",
    "print(f\"Generated {len(document_embeddings)} embeddings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79efa21f-5006-4c68-a5a9-a9d2cc6ff3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored document embeddings in FAISS vector database.\n"
     ]
    }
   ],
   "source": [
    "# Convert embeddings to NumPy array and add to FAISS index\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "dimension = len(document_embeddings[0])  # Vector size \n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "vectors = np.array(document_embeddings, dtype=\"float32\")\n",
    "index.add(vectors)\n",
    "\n",
    "print(\"Stored document embeddings in FAISS vector database.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1df8d92-311e-4d55-b9cc-2a18c51c18a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49888dcefe86496084f1c937cf49e324",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Code from https://github.com/QwenLM/Qwen2.5-VL, adjusted with 3B version\n",
    "\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import torch\n",
    "import os\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# default: Load the model on the available device(s)\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-3B-Instruct\", \n",
    "    torch_dtype=torch.bfloat16, \n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c9205a9-d95b-49fe-b261-3c4526cf9d2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 💬 AI NO RAG Response ===\n",
      "system\n",
      "Use the following information to enhance your answer.\n",
      "user\n",
      "What fruit is this? Is Brazil its largest producer?\n",
      "assistant\n",
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "What fruit is this? Is Brazil its largest producer?\n",
      "assistant\n",
      "The fruit in the picture is an orange. Oranges are indeed one of the most widely produced fruits globally, with Brazil being one of the largest producers. Brazil is known for its extensive citrus farming and is a major exporter of oranges and other citrus products.\n",
      "=======================\n"
     ]
    }
   ],
   "source": [
    "# No RAG\n",
    "from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# Load Qwen model and processor\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\")\n",
    "\n",
    "# Define user query with image\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": \"orange.jpg\",\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"What fruit is this? Is Brazil its largest producer?\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Prepare input for inference\n",
    "text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "# Generate response\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=512)\n",
    "response = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "# Function to format AI response like RAG-style output\n",
    "def format_ai_response(response_text, user_message):\n",
    "    # Structure the response in an easy-to-read format\n",
    "    formatted_response = f\"=== 💬 AI NO RAG Response ===\\n\"\n",
    "    formatted_response += \"system\\nUse the following information to enhance your answer.\\n\"\n",
    "    formatted_response += f\"user\\n{user_message}\\n\"\n",
    "    formatted_response += f\"assistant\\n{response_text.strip()}\\n\"\n",
    "    formatted_response += \"=======================\"\n",
    "    \n",
    "    return formatted_response\n",
    "\n",
    "# Extract user message text for structured output\n",
    "user_message_text = messages[0][\"content\"][1][\"text\"]\n",
    "\n",
    "# Print formatted AI response\n",
    "print(format_ai_response(response[0], user_message_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "052a67d0-c407-4cd5-af1e-40bdfd6c27a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DEBUG] Retrieved Context:\n",
      "1. Brazil is no longer the world's largest producer of oranges. It is China now, producing up to 60% of the world's orange population.\n",
      "2. Orange juice is a popular breakfast drink worldwide.\n",
      "\n",
      "=== 💬 AI with RAG Response ===\n",
      "system\n",
      "Use the following information to answer the question. Prioritize this information over other knowledge, but you may supplement with general knowledge if necessary.\n",
      "\n",
      "Result:\n",
      "user\n",
      "What fruit is this? Is Brazil still its largest producer?\n",
      "system\n",
      "Use the following information to answer the question. Prioritize this information over other knowledge, but if additional details are needed, you may supplement with general knowledge.\n",
      "user\n",
      "Context: Brazil is no longer the world's largest producer of oranges. It is China now, producing up to 60% of the world's orange population. Orange juice is a popular breakfast drink worldwide.\n",
      "\n",
      "What fruit is this? Is Brazil still its largest producer?\n",
      "assistant\n",
      "The fruit in the picture is an orange. Brazil is not the largest producer of oranges anymore; China has taken that position, producing up to 60% of the world's orange population.\n",
      "=======================\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# Function to retrieve the most relevant document based on the user's actual question\n",
    "def retrieve_relevant_text(user_question, top_k=2):\n",
    "    query_embedding = embed_model.encode(user_question).reshape(1, -1)\n",
    "    _, retrieved_indices = index.search(query_embedding, top_k)\n",
    "    retrieved_docs = [documents[i] for i in retrieved_indices[0]]\n",
    "    return retrieved_docs\n",
    "\n",
    "# SET USER QUERY HERE\n",
    "user_question = \"What fruit is this? Is Brazil still its largest producer?\"\n",
    "\n",
    "# Use the user's actual question as the query\n",
    "retrieved_context = retrieve_relevant_text(user_question)\n",
    "\n",
    "# Debug output: Print retrieved context separately\n",
    "print(\"\\n[DEBUG] Retrieved Context:\")\n",
    "for i, fact in enumerate(retrieved_context, 1):\n",
    "    print(f\"{i}. {fact}\")\n",
    "\n",
    "# Combine retrieved context into a single string for input\n",
    "retrieved_text = \" \".join(retrieved_context)\n",
    "\n",
    "# 🛠️ Prioritize dataset knowledge but allow fallback to general knowledge\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": (\n",
    "        \"Use the following information to answer the question. \"\n",
    "        \"Prioritize this information over other knowledge, but if additional details are needed, \"\n",
    "        \"you may supplement with general knowledge.\"\n",
    "    )},\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\n",
    "            \"type\": \"image\",\n",
    "            \"image\": \"orange.jpg\",\n",
    "        },\n",
    "        {\"type\": \"text\", \"text\": f\"Context: {retrieved_text}\\n\\n{user_question}\"},\n",
    "    ]}\n",
    "]\n",
    "\n",
    "# Convert to model input\n",
    "text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "# Generate response\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=512)\n",
    "response = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "# Print final output in a structured format\n",
    "print(\"\\n=== 💬 AI with RAG Response ===\")\n",
    "print(\"system\\nUse the following information to answer the question. Prioritize this information over other knowledge, but you may supplement with general knowledge if necessary.\")\n",
    "print()\n",
    "print(\"Result:\")\n",
    "print(f\"user\\n{user_question}\\n{response[0]}\")\n",
    "print(\"=======================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d278067-d26e-4532-a489-b0613f617c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GPU Memory: 24576 MB\n",
      "Used GPU Memory: 17437 MB\n",
      "Free GPU Memory: 6709 MB\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def check_cuda_memory():\n",
    "    try:\n",
    "        output = subprocess.check_output([\"nvidia-smi\", \"--query-gpu=memory.total,memory.used,memory.free\", \"--format=csv,nounits,noheader\"])\n",
    "        total, used, free = map(int, output.decode(\"utf-8\").strip().split(\"\\n\")[0].split(\", \"))\n",
    "        print(f\"Total GPU Memory: {total} MB\")\n",
    "        print(f\"Used GPU Memory: {used} MB\")\n",
    "        print(f\"Free GPU Memory: {free} MB\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "check_cuda_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcd6384-a89f-4a41-aa87-cfd5e528228d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alright, here's what I need you to do.\n",
    "# A) I need you to generate an image of a flowchart using python via mermaid, something related with business workflow\n",
    "# B) Generate a corresponding PDF file that contains refined details of said flowchart\n",
    "# C) Write the mermaid code in the PDF file as well"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
