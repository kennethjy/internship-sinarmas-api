{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "92be6b38-9e08-44e1-8cb1-02fe5acaad34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Stored document text embeddings in FAISS database.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "import fitz  \n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "### DOCUMENT PROCESSING ###\n",
    "\n",
    "# Load text embedding model\n",
    "text_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a given PDF file.\"\"\"\n",
    "    text = \"\"\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        for page in doc:\n",
    "            text += page.get_text(\"text\") + \"\\n\"\n",
    "    return text.strip()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Enhance retrieval by applying TF-IDF weighting.\"\"\"\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=5000)\n",
    "    X = vectorizer.fit_transform([text]).toarray()\n",
    "    return \" \".join(vectorizer.get_feature_names_out())\n",
    "\n",
    "# Directory setup\n",
    "base_dir = \"business_flowcharts\"\n",
    "pdf_dir = os.path.join(base_dir, \"documents\")\n",
    "\n",
    "# Read and process PDFs\n",
    "pdf_texts = {}\n",
    "pdf_filenames = sorted([f for f in os.listdir(pdf_dir) if f.endswith(\".pdf\")])\n",
    "\n",
    "pdf_embeddings = []\n",
    "for file in pdf_filenames:\n",
    "    pdf_path = os.path.join(pdf_dir, file)\n",
    "    raw_text = extract_text_from_pdf(pdf_path)\n",
    "    processed_text = preprocess_text(raw_text)\n",
    "    pdf_texts[file] = raw_text\n",
    "    embedding = text_model.encode(processed_text)\n",
    "    pdf_embeddings.append(embedding)\n",
    "\n",
    "# Convert to FAISS-compatible format\n",
    "pdf_embeddings = np.array(pdf_embeddings, dtype=\"float32\")\n",
    "pdf_embeddings /= np.linalg.norm(pdf_embeddings, axis=1, keepdims=True)  # Normalize (IMPORTANT)\n",
    "\n",
    "# Create FAISS index\n",
    "text_index = faiss.IndexFlatL2(pdf_embeddings.shape[1])\n",
    "text_index.add(pdf_embeddings)\n",
    "\n",
    "print(\"✅ Stored document text embeddings in FAISS database.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "105709b4-3c02-4b95-b64f-1bfe534ecf33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Stored flowchart image embeddings in FAISS database.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "### FLOWCHART IMG PROCESSING ###\n",
    "\n",
    "# Load CLIP model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    \"\"\"Preprocess an image dynamically while maintaining aspect ratio.\"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # Resize while keeping aspect ratio\n",
    "    aspect_ratio = image.width / image.height\n",
    "    if aspect_ratio > 1:\n",
    "        new_width = 224\n",
    "        new_height = int(224 / aspect_ratio)\n",
    "    else:\n",
    "        new_height = 224\n",
    "        new_width = int(224 * aspect_ratio)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((new_height, new_width)),  # Maintain aspect ratio\n",
    "        transforms.Pad((0, 0, 224 - new_width, 224 - new_height), fill=(255, 255, 255)),  # Pad with white\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.481, 0.457, 0.408], std=[0.268, 0.261, 0.275]),  # CLIP normalization (ALSO IMPORATNT)\n",
    "    ])\n",
    "\n",
    "    return transform(image).unsqueeze(0)\n",
    "\n",
    "def get_image_embedding(image_path):\n",
    "    \"\"\"Generate an embedding for a flowchart image using CLIP.\"\"\"\n",
    "    image_tensor = preprocess_image(image_path).to(device)\n",
    "    with torch.no_grad():\n",
    "        embedding = clip_model.encode_image(image_tensor).cpu().numpy()\n",
    "    return embedding.flatten()\n",
    "\n",
    "# Directory setup\n",
    "image_dir = os.path.join(base_dir, \"flowcharts\")\n",
    "image_filenames = sorted([f for f in os.listdir(image_dir) if f.endswith(\".png\")])\n",
    "\n",
    "# Process images\n",
    "image_embeddings = [get_image_embedding(os.path.join(image_dir, file)) for file in image_filenames]\n",
    "image_embeddings = np.array(image_embeddings, dtype=\"float32\")\n",
    "image_embeddings /= np.linalg.norm(image_embeddings, axis=1, keepdims=True)  # Normalize\n",
    "\n",
    "# Create FAISS index\n",
    "image_index = faiss.IndexFlatL2(image_embeddings.shape[1])\n",
    "image_index.add(image_embeddings)\n",
    "\n",
    "print(\"✅ Stored flowchart image embeddings in FAISS database.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c74fd01e-e942-4b97-91fa-bd33fddb297f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "\n",
    "def extract_text_from_image(image_path):\n",
    "    \"\"\"Extract text from a given flowchart image using OCR.\"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    extracted_text = pytesseract.image_to_string(image)\n",
    "    return extracted_text.strip()\n",
    "\n",
    "def get_text_embedding(text):\n",
    "    \"\"\"Generate an embedding for extracted text using Sentence-BERT.\"\"\"\n",
    "    return text_model.encode(text)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "415b3fc6-373d-4c5a-a154-cc97f6e45102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Retrieval function is ready.\n"
     ]
    }
   ],
   "source": [
    "def get_query_image_embedding(image_path):\n",
    "    \"\"\"Generate a normalized embedding for a query image using CLIP.\"\"\"\n",
    "    image_tensor = preprocess_image(image_path).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embedding = clip_model.encode_image(image_tensor).cpu().numpy()\n",
    "    \n",
    "    return embedding.flatten() / np.linalg.norm(embedding)  # Normalize\n",
    "\n",
    "\n",
    "# Updated Retrieval Function - Supports Multiple Flowchart Images and PDFs\n",
    "\n",
    "def retrieve_relevant_data(query, flowchart_imgs=[], top_k=2):\n",
    "    \"\"\"Retrieve the most relevant documents & images for the query using FAISS.\n",
    "    Supports multiple flowchart images for comparison.\"\"\"\n",
    "\n",
    "    # Convert query to text embedding\n",
    "    query_embedding = text_model.encode(query).reshape(1, -1)\n",
    "    query_embedding /= np.linalg.norm(query_embedding)  # Normalize\n",
    "\n",
    "    # Search FAISS text database (Retrieve relevant PDFs)\n",
    "    _, text_results = text_index.search(query_embedding, top_k)\n",
    "    retrieved_pdfs = [pdf_filenames[idx] for idx in text_results[0]]\n",
    "\n",
    "    # Process each flowchart image if provided\n",
    "    extracted_texts = []\n",
    "    image_embeddings_list = []\n",
    "    \n",
    "    if flowchart_imgs:\n",
    "        for img_path in flowchart_imgs:\n",
    "            extracted_text = extract_text_from_image(img_path)\n",
    "            extracted_texts.append(extracted_text)\n",
    "            text_embedding = get_text_embedding(extracted_text)\n",
    "\n",
    "            # Use extracted text to retrieve relevant documents\n",
    "            _, text_results = text_index.search(text_embedding.reshape(1, -1), top_k)\n",
    "            retrieved_pdfs.extend([pdf_filenames[idx] for idx in text_results[0]])\n",
    "\n",
    "            # Generate CLIP image embedding\n",
    "            image_query_embedding = get_query_image_embedding(img_path)\n",
    "            image_embeddings_list.append(image_query_embedding)\n",
    "\n",
    "    # Normalize embeddings for images\n",
    "    if image_embeddings_list:\n",
    "        image_query_embeddings = np.array(image_embeddings_list, dtype=\"float32\")\n",
    "        image_query_embeddings /= np.linalg.norm(image_query_embeddings, axis=1, keepdims=True)\n",
    "        \n",
    "        # Search FAISS image database\n",
    "        _, image_results = image_index.search(image_query_embeddings, top_k)\n",
    "        retrieved_images = [image_filenames[idx] for idx in image_results.flatten()]\n",
    "    else:\n",
    "        retrieved_images = []\n",
    "\n",
    "    return list(set(retrieved_pdfs)), list(set(retrieved_images)), extracted_texts\n",
    "\n",
    "print(\"✅ Retrieval function is ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "822b870f-ae8d-4c0c-9a11-ee62a0c660f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc7cbb9fff64471c9732555214ed9fbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration\n",
    "\n",
    "# Load Qwen model\n",
    "qwen_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-3B-Instruct\", \n",
    "    torch_dtype=torch.bfloat16,  \n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "qwen_processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a0054b9c-2665-4071-b16c-a30b8da66fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Updated Qwen RAG system is ready.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "# Updated Qwen RAG Query Function - Multi-Flowchart & Multi-Document Support\n",
    "\n",
    "def query_qwen_with_rag(query, flowchart_imgs=[], top_k=2):\n",
    "    \"\"\"Retrieve relevant flowchart data (text & images) and query Qwen for comparison-style answers.\"\"\"\n",
    "    \n",
    "    # Retrieve relevant PDFs (text) & Flowcharts (images)\n",
    "    retrieved_pdfs, retrieved_images, extracted_texts = retrieve_relevant_data(query, flowchart_imgs, top_k)\n",
    "    \n",
    "    # Extract text from retrieved PDFs\n",
    "    context = \"\\n\\n\".join([f\"📄 **{pdf}**\\n{pdf_texts[pdf]}\" for pdf in retrieved_pdfs])\n",
    "    \n",
    "    # Extract text from all provided flowcharts\n",
    "    flowchart_texts = \"\\n\\n\".join([f\"🖼️ **Flowchart {i+1}**\\n{text}\" for i, text in enumerate(extracted_texts)])\n",
    "    \n",
    "    # Select primary image for visualization (first one provided or retrieved)\n",
    "    primary_image_path = flowchart_imgs[0] if flowchart_imgs else os.path.join(image_dir, retrieved_images[0])\n",
    "    primary_image = Image.open(primary_image_path).convert(\"RGB\")\n",
    "    \n",
    "    # Define the user message (Injecting retrieved context)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": primary_image},  # Show one image\n",
    "                {\"type\": \"text\", \"text\": f\"\"\"\n",
    "Given the attached document and flowcharts, compare the provided flowcharts and documents. \n",
    "Analyze differences, improvements, or regressions. If the newer flowchart is better than the old one, explain why.\n",
    "\n",
    "### **Context from Retrieved Documents**\n",
    "{context}\n",
    "\n",
    "### **Extracted Text from Provided Flowcharts**\n",
    "{flowchart_texts}\n",
    "\n",
    "### **Query**\n",
    "{query}\n",
    "\"\"\"},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Format input for Qwen\n",
    "    text = qwen_processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = qwen_processor(\n",
    "        text=[text],\n",
    "        images=[primary_image],  # Only sending one image (rest used as context)\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(qwen_model.device)\n",
    "    \n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        output_ids = qwen_model.generate(**inputs, max_new_tokens=512)\n",
    "    \n",
    "    # Decode response\n",
    "    response_text = qwen_processor.batch_decode(output_ids, skip_special_tokens=False)[0]\n",
    "\n",
    "    # Extract Assistant's Response Only\n",
    "    match = re.search(r\"assistant\\s*\\n(.*)\", response_text, re.DOTALL)\n",
    "    cleaned_response = match.group(1).strip() if match else response_text.strip()\n",
    "    \n",
    "    return cleaned_response, retrieved_pdfs, retrieved_images\n",
    "\n",
    "print(\"✅ Updated Qwen RAG system is ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e8f59309-ee38-4cfb-8f92-05eb36486d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🤖 Qwen's Response (With RAG - Multi-Flowchart Comparison):\n",
      "To compare the two flowcharts and determine which one is better, let's analyze them step by step:\n",
      "\n",
      "### **Flowchart 1: Incident Management**\n",
      "\n",
      "**Steps:**\n",
      "1. Start\n",
      "2. User Reports Issue\n",
      "3. Classify Incident\n",
      "4. High Impact?\n",
      "   - No: Assign to Support Team\n",
      "   - Yes: Escalate to Higher Support\n",
      "5. Assign to Support Team\n",
      "6. Resolve Issue\n",
      "7. Issue Resolved?\n",
      "   - Yes: Close Ticket\n",
      "   - No: Escalate to Higher Support\n",
      "\n",
      "**Improvements:**\n",
      "- The flowchart clearly distinguishes between high-impact and non-high-impact incidents.\n",
      "- It provides a clear path for escalating issues to higher support levels.\n",
      "- It includes a step for confirming the issue is resolved before closing the ticket.\n",
      "\n",
      "### **Flowchart 2: Incident Management**\n",
      "\n",
      "**Steps:**\n",
      "1. Start\n",
      "2. User Reports Issue\n",
      "3. Initial Triage\n",
      "4. Classify Incident\n",
      "5. Assign to Support Team | | Escalate to Higher Support\n",
      "6. Resolve Issue\n",
      "7. Issue Resolved?\n",
      "   - Yes: Close Ticket\n",
      "   - No: Escalate to Higher Support\n",
      "\n",
      "**Improvements:**\n",
      "- The flowchart includes an \"Initial Triage\" step, which seems redundant since it appears to be a duplicate of the \"Classify Incident\" step.\n",
      "- It simplifies the decision point by combining \"High Impact?\" and \"Assign to Support Team\" into a single decision point.\n",
      "- It also combines \"Escalate to Higher Support\" with \"Assign to Support Team.\"\n",
      "\n",
      "### **Comparison and Analysis:**\n",
      "\n",
      "#### **Clarity and Simplicity:**\n",
      "- **Flowchart 1** is more straightforward and easier to follow. Each step is clearly defined, and the decision points are well-separated.\n",
      "- **Flowchart 2** has some redundancy (e.g., \"Initial Triage\" and \"Classify Incident\") and a more complex decision point structure.\n",
      "\n",
      "#### **Redundancy:**\n",
      "- **Flowchart 1** avoids unnecessary steps and decision points, making it more efficient.\n",
      "- **Flowchart 2** contains redundant steps (\"Initial Triage\" and \"Classify Incident\"), which could lead to confusion.\n",
      "\n",
      "#### **Decision Points:**\n",
      "- **Flowchart 1** uses a clear decision point for high-impact incidents, ensuring that critical issues are handled promptly.\n",
      "- **Flowchart 2** combines decision points, which might make it harder to understand at a glance\n",
      "🔍 Retrieved Documents: ['10_Help_Desk_Ticketing.pdf', '6_Incident_Management.pdf', '7_Software_Deployment.pdf']\n",
      "🖼️ Retrieved Flowcharts: ['1.png', '2.png', '6_incident_management.png', '5.png']\n",
      "⏳ Execution Time: 9.32 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# ✅ Multi-Flowchart Test Query\n",
    "\n",
    "user_query = \"Compare these flowcharts. Which one is better and why?\"\n",
    "flowchart_imgs = [\n",
    "    \"business_flowcharts/flowcharts/6_incident_management.png\",\n",
    "    \"6_incident_management_v2.png\"\n",
    "]  \n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Query Qwen with Multi-Flowchart RAG\n",
    "qwen_response_rag, retrieved_pdfs, retrieved_images = query_qwen_with_rag(user_query, flowchart_imgs)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Print results\n",
    "print(f\"\\n🤖 Qwen's Response (With RAG - Multi-Flowchart Comparison):\\n{qwen_response_rag}\")\n",
    "print(f\"🔍 Retrieved Documents: {retrieved_pdfs}\")\n",
    "print(f\"🖼️ Retrieved Flowcharts: {retrieved_images}\")\n",
    "print(f\"⏳ Execution Time: {execution_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8dea76-1747-4a9b-b046-27899ce12cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49356fd-0545-4551-a4b3-4668aa1b6b8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b93bc4-64e0-4cd4-ad25-026641b55f01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b179f88c-ba12-4c6a-917b-af91457c5eb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a50a5d-6d73-43d1-ace9-8727e880eba8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7a28f1-4ace-4e3a-829c-cd9c216c652a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3485752-10f0-4c85-99dd-181045a248ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c82926c-7ba4-47bd-86c7-4c06746ba453",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ef5851-6a7d-4f18-9e51-ab9801842d8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4e9eb7-d63a-427c-9263-bdd000411048",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26fda206-ca13-4497-9313-462900efb1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test saved documents in FAISS database\n",
    "# def view_stored_pdfs():\n",
    "#     \"\"\"Display all stored PDFs and their extracted text.\"\"\"\n",
    "#     for filename, text in pdf_texts.items():\n",
    "#         print(f\"📄 PDF: {filename}\\n\")\n",
    "#         print(f\"Extracted Content:\\n{text[:1000]}\")  # Show first 1000 characters\n",
    "#         print(\"=\"*80)\n",
    "\n",
    "# view_stored_pdfs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "766448a9-e470-49d0-9eff-3adef9c37473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Test saved images in FAISS database\n",
    "\n",
    "# def view_stored_flowcharts():\n",
    "#     \"\"\"Display all stored flowchart images and their filenames.\"\"\"\n",
    "#     for filename in image_filenames:\n",
    "#         image_path = os.path.join(image_dir, filename)\n",
    "#         image = Image.open(image_path)\n",
    "        \n",
    "#         plt.figure(figsize=(5, 5))\n",
    "#         plt.imshow(image)\n",
    "#         plt.axis(\"off\")\n",
    "#         plt.title(f\"🖼️ Flowchart: {filename}\")\n",
    "#         plt.show()\n",
    "\n",
    "# view_stored_flowcharts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9e5fcdf-793f-41ba-a300-ff851d41b6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_pdf_embedding(index=11):\n",
    "#     \"\"\"Check stored text embedding by retrieving the closest match for a given PDF.\"\"\"\n",
    "#     query_embedding = pdf_embeddings[index].reshape(1, -1)\n",
    "#     _, retrieved_indices = text_index.search(query_embedding, 1)\n",
    "    \n",
    "#     original_pdf = pdf_filenames[index]\n",
    "#     matched_pdf = pdf_filenames[retrieved_indices[0][0]]\n",
    "    \n",
    "#     print(f\"📄 Original PDF: {original_pdf}\")\n",
    "#     print(f\"🔍 Closest Match: {matched_pdf}\")\n",
    "#     print(f\"Similarity Score: {np.dot(pdf_embeddings[index], pdf_embeddings[retrieved_indices[0][0]])}\")\n",
    "    \n",
    "# check_pdf_embedding()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b23561cd-0ea7-427f-8c03-41c278131caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def check_image_embedding(index=4):\n",
    "#     \"\"\"Check stored image embedding by retrieving the closest match for a given flowchart.\"\"\"\n",
    "#     query_embedding = image_embeddings[index].reshape(1, -1)\n",
    "#     _, retrieved_indices = image_index.search(query_embedding, 1)\n",
    "    \n",
    "#     original_image = image_filenames[index]\n",
    "#     matched_image = image_filenames[retrieved_indices[0][0]]\n",
    "    \n",
    "#     print(f\"🖼️ Original Flowchart: {original_image}\")\n",
    "#     print(f\"🔍 Closest Match: {matched_image}\")\n",
    "#     print(f\"Similarity Score: {np.dot(image_embeddings[index], image_embeddings[retrieved_indices[0][0]])}\")\n",
    "    \n",
    "#     # Show both images\n",
    "#     fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    \n",
    "#     ax[0].imshow(Image.open(os.path.join(image_dir, original_image)))\n",
    "#     ax[0].set_title(\"Original Image\")\n",
    "#     ax[0].axis(\"off\")\n",
    "    \n",
    "#     ax[1].imshow(Image.open(os.path.join(image_dir, matched_image)))\n",
    "#     ax[1].set_title(\"Closest Match\")\n",
    "#     ax[1].axis(\"off\")\n",
    "    \n",
    "#     plt.show()\n",
    "\n",
    "# check_image_embedding()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ef5c1cd-0f45-4618-975d-6d379d74c79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ FAISS has 25 unique image embeddings.\n"
     ]
    }
   ],
   "source": [
    "# # Check if all image embeddings are unique\n",
    "# unique_embeddings = np.unique(image_embeddings, axis=0)\n",
    "\n",
    "# if unique_embeddings.shape[0] == 1:\n",
    "#     print(\"⚠️ WARNING: All image embeddings are identical! FAISS cannot differentiate them.\")\n",
    "# else:\n",
    "#     print(f\"✅ FAISS has {unique_embeddings.shape[0]} unique image embeddings.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "019b9bdf-1990-476b-9466-49693196db84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print shapes of stored image embeddings\n",
    "# print(f\"Stored Image Embeddings Shape: {image_embeddings.shape}\")\n",
    "\n",
    "# # Generate a query embedding for comparison\n",
    "# query_embedding = get_query_image_embedding(os.path.join(image_dir, image_filenames[0]))  # Use any image as query\n",
    "# print(f\"Query Image Embedding Shape: {query_embedding.shape}\")\n",
    "\n",
    "# # Print first stored embedding vs query embedding\n",
    "# print(f\"\\nFirst Stored Embedding:\\n{image_embeddings[0][:10]}\")  # Print first 10 values\n",
    "# print(f\"\\nQuery Embedding:\\n{query_embedding[:10]}\")  # Print first 10 values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebf60dc3-dc92-4790-affa-50e372f30d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def debug_faiss_retrieval(index=3):\n",
    "#     \"\"\"Check if FAISS is properly differentiating images.\"\"\"\n",
    "#     query_embedding = image_embeddings[index].reshape(1, -1)  # Use stored image for retrieval test\n",
    "#     _, retrieved_indices = image_index.search(query_embedding, 5)  # Top 5 results\n",
    "\n",
    "#     print(f\"🖼️ Original Flowchart: {image_filenames[index]}\")\n",
    "#     print(f\"\\n🔍 Closest Matches:\")\n",
    "#     for rank, idx in enumerate(retrieved_indices[0]):\n",
    "#         matched_image = image_filenames[idx]\n",
    "#         similarity_score = np.dot(image_embeddings[index], image_embeddings[idx])  # Cosine similarity\n",
    "#         print(f\"{rank + 1}. {matched_image} (Score: {similarity_score:.6f})\")\n",
    "\n",
    "# debug_faiss_retrieval()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
